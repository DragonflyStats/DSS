\documentclass[french]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[a4paper]{geometry}
\usepackage{babel}



\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{framed}
\begin{document}

\section*{Reproducible Research (DSS5)- Week 1 - Quiz}
\newpage
\subsection*{Question 1}

%---------------------------------------- %
\newpage
\subsection*{Question 2}


%---------------------------------------- %
\newpage
\subsection*{Question 3}


%---------------------------------------- %
\newpage
\subsection*{Question 4}
\end{document}

### Reproducible Research
## Concepts and Ideas
# Replication
# Reproducible Research: make analytic data and code available 
# so that others may reproduce findings.
# Replication <- Reproducibility -> Nothing

## Research Pipeline
# Measured Data -> Analytic Data -> Computational Results
# Data/metadata, computer code, all the steps of computational analysis
## What do we need:
# Analytic data, analytic code, documentation of code and data, 
# standard means of distribution.
## What are the players:
# Authors & Readers
## Literate(statistical) Programming
# An article is a stream of text and code
# 1. documentation language (Latex)
# 2. programming language (R)
# -> Sweave, knitr package
## Scriping your analysis
# like music, writing down score
# developing notations

## Structure of a Data Analysis
# Steps: 
# 1. Define the question
# 2. Define the ideal data set
# Goal: descriptive(population), exploratory(random sample), 
#       inferential(right population, randomly sampled), 
#       predictive(training/test), causal(randomized study), mechanistic
# 3. Determine what data you can access
# Term of Use, UCI Spambase ML data
# 4. Obtain the data
# Raw data, reference the source, record the url and time accessed
# kernlab
# 5. Clean the data
# Reformating, subsampling
library(kernlab)
data(spam)
str(spam[, 1:5])
# 6. Exploratory data analysis
set.seed(3435)
trainIndicator <- rbinom(4601, size=1, prob=0.5)
table(trainIndicator)
trainSpam <- spam[trainIndicator==1, ]
testSpam <- spam[trainIndicator==0, ]
# summaries of the data, missing data, exploratory plots
names(trainSpam)
head(trainSpam)
table(trainSpam$type)
plot(trainSpam$capitalAve ~ trainSpam$type)
plot(log10(trainSpam$capitalAve + 1) ~ trainSpam$type)
# Relationships between predictors
plot(log10(trainSpam[, 1:4] + 1))
# Clustering
hCluster <- hclust(dist(t(trainSpam[, 1:57])))
plot(hCluster)
# New clustering
hCluster <- hclust(dist(t(log(trainSpam[, 1:55]))))
plot(hCluster)
# 7. Statistical prediction/modeling
trainSpam$numType <- as.numeric(trainSpam$type) - 1
costFunction <- function(x, y) sum(x != (y > 0.5))
cvError <- rep(NA, 55)
library(boot)
for(i in 1:55) {
    lmFormula <- reformulate(names(trainSpam)[i], response="numType")
    glmFit <- glm(lmFormula, family="binomial", data=trainSpam)
    cvError[i] <- cv.glm(trainSpam, glmFit, costFunction, 2)$delta[2]
}
# which predictor has min cv-error?
names(trainSpam)[which.min(cvError)]
# use the best model from the group
predictionModel <- glm(numType ~ charDollar, family="binomial", data=trainSpam)
# get predictions on the test set
predictionTest <- predict(predictionModel, testSpam)
predictedSpam <- rep("nonspam", dim(testSpam)[1])
# classify as 'spam' for those with prob > 0.5
predictedSpam[predictionModel$fitted > 0.5] <- "spam"
# classification table
table(predictedSpam, testSpam$type)
(77+457)/(1346+457+77+456) # error rate
# 8. Interpret results
# describes, correlates with/associated with, leads to/causes, predicts
# interpret coefficients
# 9. Challenge results
# Question, Data Source, Processing, Analysis, Conclusions
# Measures of uncertainty, choices of terms to include in models
# Alternative analysis methods
# 10. Synthesize/write up results
# Lead with the question, summarize the analyses into the story
# 11. Create reproducible code

## Organizing a Data Analysis
# Data: raw data(in analysis folder), 
#       processed data(should be tidy)
# Figures: exploratory figures(don't need to be pretty), 
#          final figures(clear, multiple panels)
# R code: raw/unused scripts(less commented), 
#         final scripts(processing details, bigger commented blocks), 
#         R Markdown files(text & R are integrated)
# Text: README files(url, description, date accessed), 
#       Text of analysis/report(title, introduction, methods, 
#                               results and conclusions)
